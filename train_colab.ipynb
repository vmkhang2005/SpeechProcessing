{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds4qWhts52uL"
      },
      "source": [
        "# üéôÔ∏è Speech Denoising - Google Colab Training\n",
        "\n",
        "Notebook n√†y cho ph√©p train model Speech Denoising U-Net tr√™n Google Colab v·ªõi GPU.\n",
        "\n",
        "## Overview\n",
        "- **Model**: U-Net v·ªõi Complex Ratio Mask (CRM)\n",
        "- **Dataset**: VoiceBank + DEMAND (t·ª´ Google Drive)\n",
        "- **Training Time**: ~1-2 gi·ªù tr√™n Colab GPU (T4/P100)\n",
        "\n",
        "## C·∫•u tr√∫c Dataset tr√™n Google Drive\n",
        "```\n",
        "speech_denoising_data/\n",
        "‚îú‚îÄ‚îÄ clean_trainset_28spk_wav/   (11,572 files)\n",
        "‚îú‚îÄ‚îÄ noisy_trainset_28spk_wav/   (11,572 files)\n",
        "‚îú‚îÄ‚îÄ clean_testset_wav/          (824 files)\n",
        "‚îî‚îÄ‚îÄ noisy_testset_wav/          (824 files)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijXYBJYN52uO"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2PKV2Wx52uP",
        "outputId": "376b548a-547c-4afa-d247-aa9655d22fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.1+cu128\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNV9f4ex52uQ",
        "outputId": "8e51545a-ecbb-44e2-be3a-bdf6eec166c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (no torchaudio needed - using librosa for audio loading)\n",
        "!pip install -q torch --upgrade\n",
        "!pip install -q librosa soundfile scipy numpy pandas\n",
        "!pip install -q pystoi matplotlib seaborn tensorboard\n",
        "!pip install -q tqdm pyyaml\n",
        "\n",
        "# Optional: Install PESQ\n",
        "!pip install -q pesq || echo \"PESQ installation failed - continuing without it\"\n",
        "\n",
        "print(\"\\n‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCky3Scg52uR"
      },
      "source": [
        "## 2Ô∏è‚É£ Mount Google Drive & Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hQTlaTW52uR",
        "outputId": "17d7212f-fe29-4793-d8f7-6b634ac6f311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Dataset folder: speech_denoising_data\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# üìÅ C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N GOOGLE DRIVE\n",
        "# ============================================\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n t·ªõi folder ch·ª©a dataset tr√™n Google Drive\n",
        "# Th∆∞ m·ª•c n√†y ch·ª©a: clean_trainset_28spk_wav, noisy_trainset_28spk_wav, etc.\n",
        "GDRIVE_DATASET_FOLDER = \"speech_denoising_data\"\n",
        "\n",
        "# Folder ID t·ª´ URL (backup n·∫øu c·∫ßn)\n",
        "# URL: https://drive.google.com/drive/folders/1mDHfxtzvC-7kw0YXF0dFAcYlh7GAb2-\n",
        "GDRIVE_FOLDER_ID = \"1mDHfxtzvC-7kw0YXF0dFAcYlh7GAb2-\"\n",
        "\n",
        "print(f\"üìÇ Dataset folder: {GDRIVE_DATASET_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2B70OPT52uT",
        "outputId": "5cbc24f0-5095-47fb-e15a-35e315688c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Found dataset folder: /content/drive/MyDrive/speech_denoising_data\n",
            "\n",
            "üìÇ Contents:\n",
            "   üìÅ clean_trainset_28spk_wav: 11581 files\n",
            "   üìÅ clean_testset_wav: 347 files\n",
            "   üìÅ noisy_testset_wav: 824 files\n",
            "   üìÅ noisy_trainset_28spk_wav: 11575 files\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß\n",
        "GDRIVE_DATASET_PATH = f\"/content/drive/MyDrive/{GDRIVE_DATASET_FOLDER}\"\n",
        "\n",
        "# Verify dataset exists\n",
        "if os.path.exists(GDRIVE_DATASET_PATH):\n",
        "    print(f\"‚úÖ Found dataset folder: {GDRIVE_DATASET_PATH}\")\n",
        "    print(\"\\nüìÇ Contents:\")\n",
        "    for item in os.listdir(GDRIVE_DATASET_PATH):\n",
        "        item_path = os.path.join(GDRIVE_DATASET_PATH, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            count = len([f for f in os.listdir(item_path) if f.endswith('.wav')])\n",
        "            print(f\"   üìÅ {item}: {count} files\")\n",
        "else:\n",
        "    print(f\"‚ùå Dataset folder not found at: {GDRIVE_DATASET_PATH}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(f\"  1. Folder name is correct: {GDRIVE_DATASET_FOLDER}\")\n",
        "    print(\"  2. Dataset is in 'My Drive' root folder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiyIvNom52uT",
        "outputId": "33697ac1-6a00-4e9d-d6fa-48fbfb15c482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset verification:\n",
            "--------------------------------------------------\n",
            "  ‚úÖ Train Clean: 11581 files\n",
            "  ‚úÖ Train Noisy: 11575 files\n",
            "  ‚úÖ Test Clean: 347 files\n",
            "  ‚úÖ Test Noisy: 824 files\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Setup dataset paths\n",
        "TRAIN_CLEAN_DIR = os.path.join(GDRIVE_DATASET_PATH, \"clean_trainset_28spk_wav\")\n",
        "TRAIN_NOISY_DIR = os.path.join(GDRIVE_DATASET_PATH, \"noisy_trainset_28spk_wav\")\n",
        "TEST_CLEAN_DIR = os.path.join(GDRIVE_DATASET_PATH, \"clean_testset_wav\")\n",
        "TEST_NOISY_DIR = os.path.join(GDRIVE_DATASET_PATH, \"noisy_testset_wav\")\n",
        "\n",
        "# Verify all directories\n",
        "print(\"üìä Dataset verification:\")\n",
        "print(\"-\" * 50)\n",
        "for name, path in [(\"Train Clean\", TRAIN_CLEAN_DIR),\n",
        "                   (\"Train Noisy\", TRAIN_NOISY_DIR),\n",
        "                   (\"Test Clean\", TEST_CLEAN_DIR),\n",
        "                   (\"Test Noisy\", TEST_NOISY_DIR)]:\n",
        "    if os.path.exists(path):\n",
        "        count = len([f for f in os.listdir(path) if f.endswith('.wav')])\n",
        "        print(f\"  ‚úÖ {name}: {count} files\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {name}: NOT FOUND\")\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n ngu·ªìn (tr√™n Drive)\n",
        "source_path = GDRIVE_DATASET_PATH\n",
        "# ƒê∆∞·ªùng d·∫´n ƒë√≠ch (tr√™n m√°y ·∫£o Colab - t·ªëc ƒë·ªô cao)\n",
        "dest_path = \"/content/local_data\"\n",
        "\n",
        "print(f\"‚è≥ ƒêang copy d·ªØ li·ªáu t·ª´ {source_path} sang {dest_path}...\")\n",
        "print(\"Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 5-10 ph√∫t, vui l√≤ng ch·ªù...\")\n",
        "\n",
        "if not os.path.exists(dest_path):\n",
        "    shutil.copytree(source_path, dest_path)\n",
        "    print(\"‚úÖ Copy ho√†n t·∫•t!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Th∆∞ m·ª•c ƒë√≠ch ƒë√£ t·ªìn t·∫°i, b·ªè qua b∆∞·ªõc copy.\")\n",
        "\n",
        "# C·∫¨P NH·∫¨T L·∫†I ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU\n",
        "# S·ª≠a l·∫°i bi·∫øn GDRIVE_DATASET_PATH ƒë·ªÉ tr·ªè v·ªÅ th∆∞ m·ª•c local m·ªõi\n",
        "GDRIVE_DATASET_PATH = dest_path\n",
        "\n",
        "print(f\"üìÇ ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu m·ªõi: {GDRIVE_DATASET_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAghpOis-97k",
        "outputId": "2e0cf205-1a7f-45d8-a66c-55ff4ef3a276"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ ƒêang copy d·ªØ li·ªáu t·ª´ /content/drive/MyDrive/speech_denoising_data sang /content/local_data...\n",
            "Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 5-10 ph√∫t, vui l√≤ng ch·ªù...\n",
            "‚ö†Ô∏è Th∆∞ m·ª•c ƒë√≠ch ƒë√£ t·ªìn t·∫°i, b·ªè qua b∆∞·ªõc copy.\n",
            "üìÇ ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu m·ªõi: /content/local_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcElJcRj52uU"
      },
      "source": [
        "## 3Ô∏è‚É£ Define Dataset & Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TiuSqI752uV",
        "outputId": "c76951a0-92e3-4ffd-b45f-acbbaee79e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AudioProcessor defined!\n"
          ]
        }
      ],
      "source": [
        "# Audio Processor class (Google Colab compatible - no torchaudio)\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from typing import Tuple, Optional, Dict, List\n",
        "\n",
        "class AudioProcessor:\n",
        "    \"\"\"Audio processing utilities: STFT, iSTFT\"\"\"\n",
        "\n",
        "    def __init__(self, n_fft=512, hop_length=128, win_length=512, sample_rate=16000):\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.sample_rate = sample_rate\n",
        "        self.window = torch.hann_window(win_length)\n",
        "\n",
        "    def stft(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"STFT: [batch, samples] -> [batch, freq, time, 2]\"\"\"\n",
        "        if waveform.dim() == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "        window = self.window.to(waveform.device)\n",
        "        stft_out = torch.stft(\n",
        "            waveform, n_fft=self.n_fft, hop_length=self.hop_length,\n",
        "            win_length=self.win_length, window=window,\n",
        "            return_complex=True, center=True, pad_mode='reflect'\n",
        "        )\n",
        "        return torch.stack([stft_out.real, stft_out.imag], dim=-1)\n",
        "\n",
        "    def istft(self, stft_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"iSTFT: [batch, freq, time, 2] -> [batch, samples]\"\"\"\n",
        "        window = self.window.to(stft_tensor.device)\n",
        "        stft_complex = torch.complex(stft_tensor[..., 0], stft_tensor[..., 1])\n",
        "        return torch.istft(\n",
        "            stft_complex, n_fft=self.n_fft, hop_length=self.hop_length,\n",
        "            win_length=self.win_length, window=window,\n",
        "            center=True, return_complex=False\n",
        "        )\n",
        "\n",
        "print(\"‚úÖ AudioProcessor defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9tpwnKp52uX",
        "outputId": "f2bc0b94-10d6-4c19-bc1f-9bf7404d355a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VoiceBankDEMANDDataset defined!\n"
          ]
        }
      ],
      "source": [
        "# Dataset class (using librosa instead of torchaudio for Colab compatibility)\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "import librosa\n",
        "\n",
        "class VoiceBankDEMANDDataset(Dataset):\n",
        "    \"\"\"VoiceBank + DEMAND Dataset for Speech Denoising\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clean_dir: str,\n",
        "        noisy_dir: str,\n",
        "        sample_rate: int = 16000,\n",
        "        segment_length: int = 32000,\n",
        "        n_fft: int = 512,\n",
        "        hop_length: int = 128,\n",
        "        win_length: int = 512,\n",
        "        is_train: bool = True\n",
        "    ):\n",
        "        self.clean_dir = Path(clean_dir)\n",
        "        self.noisy_dir = Path(noisy_dir)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.segment_length = segment_length\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.audio_processor = AudioProcessor(\n",
        "            n_fft=n_fft, hop_length=hop_length,\n",
        "            win_length=win_length, sample_rate=sample_rate\n",
        "        )\n",
        "\n",
        "        # Get file list\n",
        "        self.clean_files = sorted(list(self.clean_dir.glob(\"*.wav\")))\n",
        "        self.noisy_files = sorted(list(self.noisy_dir.glob(\"*.wav\")))\n",
        "\n",
        "        # Match files by name\n",
        "        clean_names = {f.stem: f for f in self.clean_files}\n",
        "        noisy_names = {f.stem: f for f in self.noisy_files}\n",
        "        common_names = set(clean_names.keys()) & set(noisy_names.keys())\n",
        "\n",
        "        self.file_pairs = [(clean_names[n], noisy_names[n]) for n in sorted(common_names)]\n",
        "        print(f\"  Found {len(self.file_pairs)} file pairs\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clean_path, noisy_path = self.file_pairs[idx]\n",
        "\n",
        "        # Load audio using librosa (Google Colab compatible)\n",
        "        clean_np, _ = librosa.load(str(clean_path), sr=self.sample_rate, mono=True)\n",
        "        noisy_np, _ = librosa.load(str(noisy_path), sr=self.sample_rate, mono=True)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        clean_wav = torch.from_numpy(clean_np).float()\n",
        "        noisy_wav = torch.from_numpy(noisy_np).float()\n",
        "\n",
        "        # Random segment for training, full audio for validation\n",
        "        if self.is_train and len(clean_wav) > self.segment_length:\n",
        "            start = random.randint(0, len(clean_wav) - self.segment_length)\n",
        "            clean_wav = clean_wav[start:start + self.segment_length]\n",
        "            noisy_wav = noisy_wav[start:start + self.segment_length]\n",
        "        else:\n",
        "            # Pad or truncate\n",
        "            if len(clean_wav) < self.segment_length:\n",
        "                pad_len = self.segment_length - len(clean_wav)\n",
        "                clean_wav = torch.nn.functional.pad(clean_wav, (0, pad_len))\n",
        "                noisy_wav = torch.nn.functional.pad(noisy_wav, (0, pad_len))\n",
        "            else:\n",
        "                clean_wav = clean_wav[:self.segment_length]\n",
        "                noisy_wav = noisy_wav[:self.segment_length]\n",
        "\n",
        "        # Compute STFT\n",
        "        clean_stft = self.audio_processor.stft(clean_wav).squeeze(0)\n",
        "        noisy_stft = self.audio_processor.stft(noisy_wav).squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'clean': clean_wav,\n",
        "            'noisy': noisy_wav,\n",
        "            'clean_stft': clean_stft,\n",
        "            'noisy_stft': noisy_stft,\n",
        "            'filename': clean_path.stem\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ VoiceBankDEMANDDataset defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNpon-lA52uX",
        "outputId": "3ef6cf37-c7bf-4171-d301-a8ad9b2c3b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ UNetDenoiser defined!\n"
          ]
        }
      ],
      "source": [
        "# U-Net Model\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Conv block with BatchNorm and LeakyReLU\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Encoder block with downsampling\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = ConvBlock(in_ch, out_ch, dropout=dropout)\n",
        "        self.conv2 = ConvBlock(out_ch, out_ch, dropout=dropout)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.pool(x), x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Decoder block with upsampling and skip connection\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
        "        self.conv1 = ConvBlock(out_ch * 2, out_ch, dropout=dropout)\n",
        "        self.conv2 = ConvBlock(out_ch, out_ch, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        # Handle size mismatch\n",
        "        if x.shape != skip.shape:\n",
        "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return self.conv2(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
        "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
        "        self.value = nn.Conv2d(channels, channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
        "        k = self.key(x).view(B, -1, H * W)\n",
        "        v = self.value(x).view(B, -1, H * W)\n",
        "\n",
        "        attn = F.softmax(torch.bmm(q, k), dim=-1)\n",
        "        out = torch.bmm(v, attn.permute(0, 2, 1)).view(B, C, H, W)\n",
        "        return self.gamma * out + x\n",
        "\n",
        "class UNetDenoiser(nn.Module):\n",
        "    \"\"\"U-Net for speech denoising with Complex Ratio Mask\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=2,\n",
        "        out_channels=2,\n",
        "        encoder_channels=[32, 64, 128, 256, 512],\n",
        "        use_attention=True,\n",
        "        dropout=0.1,\n",
        "        mask_type='CRM'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mask_type = mask_type\n",
        "\n",
        "        # Encoder\n",
        "        self.encoders = nn.ModuleList()\n",
        "        in_ch = in_channels\n",
        "        for out_ch in encoder_channels:\n",
        "            self.encoders.append(EncoderBlock(in_ch, out_ch, dropout))\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            ConvBlock(encoder_channels[-1], encoder_channels[-1] * 2, dropout=dropout),\n",
        "            AttentionBlock(encoder_channels[-1] * 2) if use_attention else nn.Identity(),\n",
        "            ConvBlock(encoder_channels[-1] * 2, encoder_channels[-1] * 2, dropout=dropout)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoders = nn.ModuleList()\n",
        "        decoder_channels = encoder_channels[::-1]\n",
        "        in_ch = encoder_channels[-1] * 2\n",
        "        for out_ch in decoder_channels:\n",
        "            self.decoders.append(DecoderBlock(in_ch, out_ch, dropout))\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Output\n",
        "        self.output = nn.Conv2d(encoder_channels[0], out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store input for mask application\n",
        "        input_stft = x\n",
        "\n",
        "        # Encoder\n",
        "        skips = []\n",
        "        for encoder in self.encoders:\n",
        "            x, skip = encoder(x)\n",
        "            skips.append(skip)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder\n",
        "        for decoder, skip in zip(self.decoders, reversed(skips)):\n",
        "            x = decoder(x, skip)\n",
        "\n",
        "        # Output mask\n",
        "        mask = self.output(x)\n",
        "\n",
        "        # Apply Complex Ratio Mask\n",
        "        if self.mask_type == 'CRM':\n",
        "            mask = torch.tanh(mask)\n",
        "            output = input_stft * mask\n",
        "        else:\n",
        "            output = mask\n",
        "\n",
        "        return output\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"‚úÖ UNetDenoiser defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvrUDPF652uZ",
        "outputId": "9189e3d4-a94f-4d1b-965a-cd63b6a14e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loss functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Loss functions\n",
        "class MultiResolutionSTFTLoss(nn.Module):\n",
        "    \"\"\"Multi-resolution STFT loss\"\"\"\n",
        "    def __init__(self, fft_sizes=[512, 1024, 2048], hop_sizes=[128, 256, 512], win_lengths=[512, 1024, 2048]):\n",
        "        super().__init__()\n",
        "        self.fft_sizes = fft_sizes\n",
        "        self.hop_sizes = hop_sizes\n",
        "        self.win_lengths = win_lengths\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        loss = 0\n",
        "        for fft_size, hop_size, win_length in zip(self.fft_sizes, self.hop_sizes, self.win_lengths):\n",
        "            window = torch.hann_window(win_length).to(pred.device)\n",
        "\n",
        "            pred_stft = torch.stft(pred, fft_size, hop_size, win_length, window, return_complex=True)\n",
        "            target_stft = torch.stft(target, fft_size, hop_size, win_length, window, return_complex=True)\n",
        "\n",
        "            pred_mag = pred_stft.abs()\n",
        "            target_mag = target_stft.abs()\n",
        "\n",
        "            # Spectral convergence + Log magnitude loss\n",
        "            loss += torch.norm(target_mag - pred_mag, p='fro') / (torch.norm(target_mag, p='fro') + 1e-8)\n",
        "            loss += F.l1_loss(torch.log(pred_mag + 1e-8), torch.log(target_mag + 1e-8))\n",
        "\n",
        "        return loss / len(self.fft_sizes)\n",
        "\n",
        "class DenoiserLoss(nn.Module):\n",
        "    \"\"\"Combined loss for speech denoising\"\"\"\n",
        "    def __init__(self, complex_weight=1.0, magnitude_weight=1.0, stft_weight=0.5, n_fft=512, hop_length=128, win_length=512, use_mr_stft=True):\n",
        "        super().__init__()\n",
        "        self.complex_weight = complex_weight\n",
        "        self.magnitude_weight = magnitude_weight\n",
        "        self.stft_weight = stft_weight\n",
        "        self.use_mr_stft = use_mr_stft\n",
        "\n",
        "        if use_mr_stft:\n",
        "            self.mr_stft_loss = MultiResolutionSTFTLoss()\n",
        "\n",
        "    def forward(self, pred_stft, target_stft, pred_wav=None, target_wav=None):\n",
        "        losses = {}\n",
        "\n",
        "        # Complex L1 loss\n",
        "        complex_loss = F.l1_loss(pred_stft, target_stft)\n",
        "        losses['complex_loss'] = complex_loss\n",
        "\n",
        "        # Magnitude loss\n",
        "        pred_mag = torch.sqrt(pred_stft[:, 0]**2 + pred_stft[:, 1]**2 + 1e-8)\n",
        "        target_mag = torch.sqrt(target_stft[:, 0]**2 + target_stft[:, 1]**2 + 1e-8)\n",
        "        magnitude_loss = F.l1_loss(pred_mag, target_mag)\n",
        "        losses['magnitude_loss'] = magnitude_loss\n",
        "\n",
        "        # MR-STFT loss\n",
        "        stft_loss = torch.tensor(0.0, device=pred_stft.device)\n",
        "        if self.use_mr_stft and pred_wav is not None and target_wav is not None:\n",
        "            stft_loss = self.mr_stft_loss(pred_wav, target_wav)\n",
        "            losses['stft_loss'] = stft_loss\n",
        "\n",
        "        # Total loss\n",
        "        total = self.complex_weight * complex_loss + self.magnitude_weight * magnitude_loss + self.stft_weight * stft_loss\n",
        "        losses['total_loss'] = total\n",
        "\n",
        "        return losses\n",
        "\n",
        "print(\"‚úÖ Loss functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfSOGGmA52ua",
        "outputId": "55ff7076-9d75-4dc7-b801-8955186fc3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Metrics defined!\n"
          ]
        }
      ],
      "source": [
        "# Metrics\n",
        "from pystoi import stoi\n",
        "\n",
        "def calculate_si_sdr(reference, estimation):\n",
        "    \"\"\"Calculate Scale-Invariant SDR\"\"\"\n",
        "    reference = reference - reference.mean()\n",
        "    estimation = estimation - estimation.mean()\n",
        "\n",
        "    dot = (reference * estimation).sum()\n",
        "    s_target = dot * reference / (reference ** 2).sum()\n",
        "    e_noise = estimation - s_target\n",
        "\n",
        "    si_sdr = 10 * torch.log10((s_target ** 2).sum() / ((e_noise ** 2).sum() + 1e-8) + 1e-8)\n",
        "    return si_sdr.item()\n",
        "\n",
        "def evaluate_batch(clean_wav, pred_wav, sample_rate=16000, compute_pesq=False, compute_stoi=True):\n",
        "    \"\"\"Evaluate batch of audio samples\"\"\"\n",
        "    metrics = {'stoi': 0.0, 'si_sdr': 0.0}\n",
        "    batch_size = clean_wav.shape[0]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        clean = clean_wav[i].cpu().numpy()\n",
        "        pred = pred_wav[i].cpu().numpy()\n",
        "\n",
        "        # STOI\n",
        "        if compute_stoi:\n",
        "            try:\n",
        "                metrics['stoi'] += stoi(clean, pred, sample_rate, extended=False)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # SI-SDR\n",
        "        metrics['si_sdr'] += calculate_si_sdr(\n",
        "            torch.from_numpy(clean),\n",
        "            torch.from_numpy(pred)\n",
        "        )\n",
        "\n",
        "    # Average\n",
        "    for key in metrics:\n",
        "        metrics[key] /= batch_size\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"‚úÖ Metrics defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITWwx1L852ua"
      },
      "source": [
        "## 4Ô∏è‚É£ Configuration & Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyVGQS2G52ub",
        "outputId": "3b6db728-3d89-4dce-ba66-e1df6275d759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Configuration:\n",
            "  Batch size: 8\n",
            "  Epochs: 50\n",
            "  Learning rate: 0.0001\n",
            "  Mixed precision: True\n"
          ]
        }
      ],
      "source": [
        "# Training configuration (optimized for Colab GPU)\n",
        "CONFIG = {\n",
        "    'data': {\n",
        "        'sample_rate': 16000,\n",
        "        'segment_length': 32000,  # 2 seconds\n",
        "    },\n",
        "    'stft': {\n",
        "        'n_fft': 512,\n",
        "        'hop_length': 128,\n",
        "        'win_length': 512,\n",
        "    },\n",
        "    'model': {\n",
        "        'encoder_channels': [32, 64, 128, 256, 512],\n",
        "        'use_attention': True,\n",
        "        'dropout': 0.1,\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 8,        # Ph√π h·ª£p v·ªõi GPU memory\n",
        "        'num_epochs': 50,       # TƒÉng l√™n 100 n·∫øu c√≥ th·ªùi gian\n",
        "        'learning_rate': 0.0001,\n",
        "        'weight_decay': 1e-5,\n",
        "        'grad_clip': 5.0,\n",
        "        'num_workers': 2,\n",
        "        'use_amp': True,        # Mixed precision\n",
        "        'early_stopping_patience': 10,\n",
        "    },\n",
        "    'scheduler': {\n",
        "        'patience': 5,\n",
        "        'factor': 0.5,\n",
        "        'min_lr': 1e-6,\n",
        "    },\n",
        "    'loss': {\n",
        "        'complex_weight': 1.0,\n",
        "        'magnitude_weight': 1.0,\n",
        "        'stft_weight': 0.5,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
        "print(f\"  Epochs: {CONFIG['training']['num_epochs']}\")\n",
        "print(f\"  Learning rate: {CONFIG['training']['learning_rate']}\")\n",
        "print(f\"  Mixed precision: {CONFIG['training']['use_amp']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgquAqlw52uc",
        "outputId": "c84a0713-6b51-4b86-cd27-7746089f5739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading dataset from Google Drive...\n",
            "\n",
            "  Loading training set...\n",
            "  Found 11572 file pairs\n",
            "  Loading validation set...\n",
            "  Found 347 file pairs\n",
            "\n",
            "‚úÖ Data loaded!\n",
            "   Training samples: 11572\n",
            "   Validation samples: 347\n",
            "   Training batches: 1446\n",
            "   Validation batches: 44\n"
          ]
        }
      ],
      "source": [
        "# Create datasets and dataloaders\n",
        "print(\"üìÇ Loading dataset from Google Drive...\")\n",
        "\n",
        "stft_cfg = CONFIG['stft']\n",
        "data_cfg = CONFIG['data']\n",
        "train_cfg = CONFIG['training']\n",
        "\n",
        "# Training dataset\n",
        "print(\"\\n  Loading training set...\")\n",
        "train_dataset = VoiceBankDEMANDDataset(\n",
        "    clean_dir=TRAIN_CLEAN_DIR,\n",
        "    noisy_dir=TRAIN_NOISY_DIR,\n",
        "    sample_rate=data_cfg['sample_rate'],\n",
        "    segment_length=data_cfg['segment_length'],\n",
        "    n_fft=stft_cfg['n_fft'],\n",
        "    hop_length=stft_cfg['hop_length'],\n",
        "    win_length=stft_cfg['win_length'],\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "# Validation dataset\n",
        "print(\"  Loading validation set...\")\n",
        "val_dataset = VoiceBankDEMANDDataset(\n",
        "    clean_dir=TEST_CLEAN_DIR,\n",
        "    noisy_dir=TEST_NOISY_DIR,\n",
        "    sample_rate=data_cfg['sample_rate'],\n",
        "    segment_length=data_cfg['segment_length'],\n",
        "    n_fft=stft_cfg['n_fft'],\n",
        "    hop_length=stft_cfg['hop_length'],\n",
        "    win_length=stft_cfg['win_length'],\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=train_cfg['num_workers'],\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=train_cfg['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Data loaded!\")\n",
        "print(f\"   Training samples: {len(train_dataset)}\")\n",
        "print(f\"   Validation samples: {len(val_dataset)}\")\n",
        "print(f\"   Training batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJyxDork52ud",
        "outputId": "9d4e5a73-be2b-4e49-f736-be8d5246397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Model: UNetDenoiser\n",
            "   Parameters: 32,418,275\n",
            "   Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cfg = CONFIG['model']\n",
        "\n",
        "model = UNetDenoiser(\n",
        "    in_channels=2,\n",
        "    out_channels=2,\n",
        "    encoder_channels=model_cfg['encoder_channels'],\n",
        "    use_attention=model_cfg['use_attention'],\n",
        "    dropout=model_cfg['dropout'],\n",
        "    mask_type='CRM'\n",
        ").to(device)\n",
        "\n",
        "print(f\"üß† Model: UNetDenoiser\")\n",
        "print(f\"   Parameters: {model.count_parameters():,}\")\n",
        "print(f\"   Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-mbbsco52ud",
        "outputId": "ac96fb47-6a27-4fe4-8bc9-737b5fdc8273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training components initialized!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1320809141.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if train_cfg['use_amp'] else None\n"
          ]
        }
      ],
      "source": [
        "# Initialize training components\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Directories\n",
        "ckpt_dir = Path('./checkpoints')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Loss function\n",
        "loss_cfg = CONFIG['loss']\n",
        "criterion = DenoiserLoss(\n",
        "    complex_weight=loss_cfg['complex_weight'],\n",
        "    magnitude_weight=loss_cfg['magnitude_weight'],\n",
        "    stft_weight=loss_cfg['stft_weight'],\n",
        "    use_mr_stft=True\n",
        ").to(device)\n",
        "\n",
        "# Audio processor for iSTFT\n",
        "audio_processor = AudioProcessor(\n",
        "    n_fft=stft_cfg['n_fft'],\n",
        "    hop_length=stft_cfg['hop_length'],\n",
        "    win_length=stft_cfg['win_length']\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_cfg['learning_rate'],\n",
        "    weight_decay=train_cfg['weight_decay']\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "scheduler_cfg = CONFIG['scheduler']\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=scheduler_cfg['factor'],\n",
        "    patience=scheduler_cfg['patience'],\n",
        "    min_lr=scheduler_cfg['min_lr']\n",
        ")\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = GradScaler() if train_cfg['use_amp'] else None\n",
        "\n",
        "print(\"‚úÖ Training components initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4jUfOfJ52ud"
      },
      "source": [
        "## 5Ô∏è‚É£ Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf3vvdVr52ud",
        "outputId": "b05be7a8-efec-4488-eae3-f1296aa4ec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Training functions\n",
        "def train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler=None):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in pbar:\n",
        "        noisy_stft = batch['noisy_stft'].to(device)\n",
        "        clean_stft = batch['clean_stft'].to(device)\n",
        "        clean_wav = batch['clean'].to(device)\n",
        "\n",
        "        # Reshape: [batch, freq, time, 2] -> [batch, 2, freq, time]\n",
        "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
        "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                pred_stft = model(noisy_stft)\n",
        "\n",
        "                # Reconstruct waveform\n",
        "                pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
        "                pred_wav = audio_processor.istft(pred_stft_istft)\n",
        "\n",
        "                # Ensure same length\n",
        "                min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
        "                pred_wav = pred_wav[..., :min_len]\n",
        "                clean_wav_trim = clean_wav[..., :min_len]\n",
        "\n",
        "                losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
        "\n",
        "            scaler.scale(losses['total_loss']).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            pred_stft = model(noisy_stft)\n",
        "            pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
        "            pred_wav = audio_processor.istft(pred_stft_istft)\n",
        "\n",
        "            min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
        "            pred_wav = pred_wav[..., :min_len]\n",
        "            clean_wav_trim = clean_wav[..., :min_len]\n",
        "\n",
        "            losses = criterion(pred_stft, clean_stft, pred_wav, clean_wav_trim)\n",
        "            losses['total_loss'].backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), train_cfg['grad_clip'])\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += losses['total_loss'].item()\n",
        "        num_batches += 1\n",
        "        pbar.set_postfix({'loss': f\"{losses['total_loss'].item():.4f}\"})\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader, criterion, audio_processor, device):\n",
        "    \"\"\"Validate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    metrics = {'stoi': 0, 'si_sdr': 0}\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "        noisy_stft = batch['noisy_stft'].to(device)\n",
        "        clean_stft = batch['clean_stft'].to(device)\n",
        "        clean_wav = batch['clean'].to(device)\n",
        "\n",
        "        noisy_stft = noisy_stft.permute(0, 3, 1, 2)\n",
        "        clean_stft = clean_stft.permute(0, 3, 1, 2)\n",
        "\n",
        "        pred_stft = model(noisy_stft)\n",
        "        pred_stft_istft = pred_stft.permute(0, 2, 3, 1)\n",
        "        pred_wav = audio_processor.istft(pred_stft_istft)\n",
        "\n",
        "        min_len = min(pred_wav.shape[-1], clean_wav.shape[-1])\n",
        "        pred_wav = pred_wav[..., :min_len]\n",
        "        clean_wav_trim = clean_wav[..., :min_len]\n",
        "\n",
        "        losses = criterion(pred_stft, clean_stft)\n",
        "        total_loss += losses['total_loss'].item()\n",
        "\n",
        "        # Metrics\n",
        "        try:\n",
        "            batch_metrics = evaluate_batch(\n",
        "                clean_wav_trim, pred_wav,\n",
        "                sample_rate=CONFIG['data']['sample_rate'],\n",
        "                compute_stoi=True\n",
        "            )\n",
        "            for key in metrics:\n",
        "                if key in batch_metrics:\n",
        "                    metrics[key] += batch_metrics[key]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_metrics = {k: v / num_batches for k, v in metrics.items()}\n",
        "\n",
        "    return avg_loss, avg_metrics\n",
        "\n",
        "print(\"‚úÖ Training functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n t·ªõi file checkpoint.\n",
        "# ∆Øu ti√™n l·∫•y best_model (k·∫øt qu·∫£ t·ªët nh·∫•t) ho·∫∑c checkpoint g·∫ßn nh·∫•t (v√≠ d·ª• epoch 15)\n",
        "resume_path = ckpt_dir / 'best_model.pt'\n",
        "\n",
        "if resume_path.exists():\n",
        "    print(f\"üîÑ ƒêang kh√¥i ph·ª•c training t·ª´: {resume_path}\")\n",
        "    checkpoint = torch.load(resume_path)\n",
        "\n",
        "    # Load tr·ªçng s·ªë model v√† optimizer\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # L·∫•y epoch ƒë√£ ch·∫°y xong\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "\n",
        "    print(f\"‚úÖ Kh√¥i ph·ª•c th√†nh c√¥ng! S·∫Ω train ti·∫øp t·ª´ Epoch {start_epoch + 1}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y checkpoint. S·∫Ω train t·ª´ ƒë·∫ßu (Epoch 1).\")\n",
        "    start_epoch = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_wwGHLVW_4v",
        "outputId": "ff1c3e09-038c-4007-8c57-56e3284c159a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ ƒêang kh√¥i ph·ª•c training t·ª´: checkpoints/best_model.pt\n",
            "‚úÖ Kh√¥i ph·ª•c th√†nh c√¥ng! S·∫Ω train ti·∫øp t·ª´ Epoch 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634,
          "referenced_widgets": [
            "7dbb3435dc7f4e15a16cfc3c96eba706",
            "4ba3927025614e33b312fef7e22b0851",
            "2eddce6bb497489b89fd4e5b64a6cd49",
            "a267fc18a4ac4bbc91c258c6f4502d9e",
            "eb786f65164447af898f267a40d102b2",
            "aac116dc7d7044e7ac005497283b8211",
            "0c1fa68cee6240359a9060fc833aed60",
            "159484c46a9743219bc2d8b5a42c67d7",
            "7bea674fa5844207908b4c3bb532491d",
            "5436f9ba1ca24621ae697d1abb3f686e",
            "d3593ebea83e42508d143b700a4800f6"
          ]
        },
        "id": "Ly3yNJ4y52ue",
        "outputId": "8923918d-1429-420c-bf14-69dca6d85b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ STARTING TRAINING\n",
            "============================================================\n",
            "Dataset: Google Drive - speech_denoising_data\n",
            "Epochs: 50\n",
            "Batch size: 8\n",
            "Device: cuda\n",
            "\n",
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1446 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dbb3435dc7f4e15a16cfc3c96eba706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2734764950.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2974978961.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2734764950.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, audio_processor, device, scaler)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnoisy_stft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noisy_stft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclean_stft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_stft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main training loop\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset: Google Drive - {GDRIVE_DATASET_FOLDER}\")\n",
        "print(f\"Epochs: {train_cfg['num_epochs']}\")\n",
        "print(f\"Batch size: {train_cfg['batch_size']}\")\n",
        "print(f\"Device: {device}\")\n",
        "print()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'stoi': [], 'si_sdr': []}\n",
        "\n",
        "for epoch in range(start_epoch, train_cfg['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{train_cfg['num_epochs']}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, audio_processor, device, scaler)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_metrics = validate(model, val_loader, criterion, audio_processor, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['stoi'].append(val_metrics.get('stoi', 0))\n",
        "    history['si_sdr'].append(val_metrics.get('si_sdr', 0))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  STOI: {val_metrics.get('stoi', 0):.3f}\")\n",
        "    print(f\"  SI-SDR: {val_metrics.get('si_sdr', 0):.2f} dB\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    # Check for best model\n",
        "    is_best = val_loss < best_val_loss\n",
        "    if is_best:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'config': CONFIG\n",
        "        }, ckpt_dir / 'best_model.pt')\n",
        "        print(\"  ‚úÖ Saved best model!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Save periodic checkpoint\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, ckpt_dir / f'checkpoint_epoch_{epoch+1}.pt')\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= train_cfg['early_stopping_patience']:\n",
        "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETED!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {ckpt_dir / 'best_model.pt'}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uuw8ut0U52ue"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history['train_loss'], label='Train')\n",
        "axes[0, 0].plot(history['val_loss'], label='Validation')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training & Validation Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# STOI\n",
        "axes[0, 1].plot(history['stoi'])\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('STOI')\n",
        "axes[0, 1].set_title('STOI (Speech Intelligibility)')\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# SI-SDR\n",
        "axes[1, 0].plot(history['si_sdr'])\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('SI-SDR (dB)')\n",
        "axes[1, 0].set_title('SI-SDR (Signal Quality)')\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# Summary\n",
        "axes[1, 1].axis('off')\n",
        "axes[1, 1].text(0.5, 0.5, f'Best Val Loss: {best_val_loss:.4f}\\n\\n'\n",
        "                f'Final STOI: {history[\"stoi\"][-1]:.3f}\\n'\n",
        "                f'Final SI-SDR: {history[\"si_sdr\"][-1]:.2f} dB',\n",
        "                ha='center', va='center', fontsize=14,\n",
        "                transform=axes[1, 1].transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Training history saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWIB3p2r52ue"
      },
      "source": [
        "## 6Ô∏è‚É£ Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIV10K2U52ue"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(ckpt_dir / 'best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
        "print(f\"   Validation loss: {checkpoint['val_loss']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_OM1DCL52ue"
      },
      "outputs": [],
      "source": [
        "# Test on a sample\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Get a test sample\n",
        "test_batch = next(iter(val_loader))\n",
        "noisy_wav = test_batch['noisy'][0:1].to(device)\n",
        "clean_wav = test_batch['clean'][0:1]\n",
        "noisy_stft = test_batch['noisy_stft'][0:1].to(device)\n",
        "\n",
        "# Denoise\n",
        "with torch.no_grad():\n",
        "    noisy_stft_input = noisy_stft.permute(0, 3, 1, 2)\n",
        "    pred_stft = model(noisy_stft_input)\n",
        "    pred_stft_out = pred_stft.permute(0, 2, 3, 1)\n",
        "    denoised_wav = audio_processor.istft(pred_stft_out)\n",
        "\n",
        "# Convert to numpy\n",
        "noisy_np = noisy_wav[0].cpu().numpy()\n",
        "clean_np = clean_wav[0].numpy()\n",
        "denoised_np = denoised_wav[0].cpu().numpy()\n",
        "\n",
        "# Ensure same length\n",
        "min_len = min(len(noisy_np), len(clean_np), len(denoised_np))\n",
        "noisy_np = noisy_np[:min_len]\n",
        "clean_np = clean_np[:min_len]\n",
        "denoised_np = denoised_np[:min_len]\n",
        "\n",
        "print(\"üéß Audio Comparison:\")\n",
        "print(\"\\n1. Noisy Input:\")\n",
        "ipd.display(ipd.Audio(noisy_np, rate=CONFIG['data']['sample_rate']))\n",
        "\n",
        "print(\"\\n2. Denoised Output:\")\n",
        "ipd.display(ipd.Audio(denoised_np, rate=CONFIG['data']['sample_rate']))\n",
        "\n",
        "print(\"\\n3. Clean Reference:\")\n",
        "ipd.display(ipd.Audio(clean_np, rate=CONFIG['data']['sample_rate']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A_AbKph52ue"
      },
      "outputs": [],
      "source": [
        "# Visualize spectrograms\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, (audio, title) in zip(axes, [(noisy_np, 'Noisy'), (denoised_np, 'Denoised'), (clean_np, 'Clean')]):\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=CONFIG['data']['sample_rate'], hop_length=128,\n",
        "                            x_axis='time', y_axis='hz', ax=ax)\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylim(0, 8000)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('spectrogram_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Spectrogram comparison saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML1_HlFz52uf"
      },
      "source": [
        "## 7Ô∏è‚É£ Save Model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_j8IL5z52uf"
      },
      "outputs": [],
      "source": [
        "# Save model to Google Drive\n",
        "import shutil\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c l∆∞u model tr√™n Google Drive\n",
        "GDRIVE_MODEL_SAVE_PATH = \"/content/drive/MyDrive/speech_denoising_models\"\n",
        "save_path = Path(GDRIVE_MODEL_SAVE_PATH)\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy best model\n",
        "shutil.copy(ckpt_dir / 'best_model.pt', save_path / 'best_model.pt')\n",
        "\n",
        "# Copy training history\n",
        "if Path('training_history.png').exists():\n",
        "    shutil.copy('training_history.png', save_path / 'training_history.png')\n",
        "if Path('spectrogram_comparison.png').exists():\n",
        "    shutil.copy('spectrogram_comparison.png', save_path / 'spectrogram_comparison.png')\n",
        "\n",
        "print(f\"‚úÖ Model saved to Google Drive: {save_path}\")\n",
        "print(\"   Files saved:\")\n",
        "for f in save_path.iterdir():\n",
        "    print(f\"   - {f.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxGfgfb352uf"
      },
      "outputs": [],
      "source": [
        "# Optional: Download to local machine\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üì• Downloading trained model...\")\n",
        "files.download(str(ckpt_dir / 'best_model.pt'))\n",
        "print(\"\\n‚úÖ Download started! Check your browser downloads.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gycWCFfM52uf"
      },
      "source": [
        "---\n",
        "\n",
        "## üìù Notes\n",
        "\n",
        "### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng\n",
        "1. Upload dataset l√™n Google Drive v·ªõi c·∫•u tr√∫c th∆∞ m·ª•c ƒë√∫ng\n",
        "2. S·ª≠a `GDRIVE_DATASET_FOLDER` n·∫øu t√™n th∆∞ m·ª•c kh√°c\n",
        "3. Ch·∫°y t·ª´ng cell t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi\n",
        "4. Model s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o Google Drive sau khi train xong\n",
        "\n",
        "### Training Tips\n",
        "- **Th·ªùi gian**: ~1-2 gi·ªù tr√™n Colab GPU (T4) cho 50 epochs\n",
        "- **Memory**: Model s·ª≠ d·ª•ng ~4-6GB GPU memory v·ªõi batch size 8\n",
        "- **TƒÉng epochs**: ƒê·ªïi `num_epochs` th√†nh 100 ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n\n",
        "- Check GPU: `!nvidia-smi`\n",
        "\n",
        "### Sau khi train\n",
        "- Model ƒë∆∞·ª£c l∆∞u t·∫°i `./checkpoints/best_model.pt`\n",
        "- Section 7 s·∫Ω copy model l√™n Google Drive ƒë·ªÉ l∆∞u tr·ªØ\n",
        "- C√≥ th·ªÉ download model v·ªÅ m√°y local"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7dbb3435dc7f4e15a16cfc3c96eba706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ba3927025614e33b312fef7e22b0851",
              "IPY_MODEL_2eddce6bb497489b89fd4e5b64a6cd49",
              "IPY_MODEL_a267fc18a4ac4bbc91c258c6f4502d9e"
            ],
            "layout": "IPY_MODEL_eb786f65164447af898f267a40d102b2"
          }
        },
        "4ba3927025614e33b312fef7e22b0851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac116dc7d7044e7ac005497283b8211",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c1fa68cee6240359a9060fc833aed60",
            "value": "Training:‚Äá‚Äá‚Äá1%"
          }
        },
        "2eddce6bb497489b89fd4e5b64a6cd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159484c46a9743219bc2d8b5a42c67d7",
            "max": 1446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bea674fa5844207908b4c3bb532491d",
            "value": 14
          }
        },
        "a267fc18a4ac4bbc91c258c6f4502d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5436f9ba1ca24621ae697d1abb3f686e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d3593ebea83e42508d143b700a4800f6",
            "value": "‚Äá14/1446‚Äá[01:42&lt;09:26,‚Äá‚Äá2.53it/s,‚Äáloss=0.6147]"
          }
        },
        "eb786f65164447af898f267a40d102b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac116dc7d7044e7ac005497283b8211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1fa68cee6240359a9060fc833aed60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159484c46a9743219bc2d8b5a42c67d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bea674fa5844207908b4c3bb532491d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5436f9ba1ca24621ae697d1abb3f686e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3593ebea83e42508d143b700a4800f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}